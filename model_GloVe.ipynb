{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words: GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fertig\n",
      "fertig\n"
     ]
    }
   ],
   "source": [
    "path1 = 'text-formality-classifier/data/formal.txt'\n",
    "with open(path1) as f:\n",
    "    corpus_formal = f.readlines()\n",
    "print(\"fertig\")\n",
    "\n",
    "path2 = 'text-formality-classifier/data/informal.txt'\n",
    "with open(path2) as f:\n",
    "    corpus_informal = f.readlines()\n",
    "print(\"fertig\")\n",
    "#%%\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(corpus_formal+corpus_informal)\n",
    "y = np.array([0]*len(corpus_formal) + [1]*len(corpus_informal) )\n",
    "\n",
    "X, y = shuffle(X,y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_t, X_te, y_t, y_te = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=0)\n",
    "#%%'\n",
    "all_train = dict(classes=[0, 1], #formal 0, informal 1\n",
    "                data = X_t,\n",
    "                categories=np.array(y_t))\n",
    "\n",
    "all_test = dict(classes=[0, 1], #formal 0, informal 1\n",
    "                data = X_te,\n",
    "                categories=np.array(y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared ：）\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_train,data_test = all_train['data'], all_test['data'] #list of strings\n",
    "label_train, label_test =all_train['categories'], all_test['categories'] #array\n",
    "print('Data prepared ：）')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84152,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84152,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21038,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors...\n",
      "400000 word vectors prepared ：）\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors...')\n",
    "\n",
    "words_index = {}\n",
    "f = open('glove.6B.100d.txt',encoding='utf-8')\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "    vector = np.asarray(word_vector[1:], dtype='float32')\n",
    "    words_index[word] = vector\n",
    "f.close()\n",
    " \n",
    "print('%s word vectors prepared ：）'%len(words_index)) #400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "# import warnings simplefilter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Found 33067 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#features\n",
    "\n",
    "print('Tokenizing...')\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "sequences = tokenizer.texts_to_sequences(data_train)\n",
    "tokenizer.fit_on_texts(data_test)\n",
    "sequences_test = tokenizer.texts_to_sequences(data_test)\n",
    "\n",
    "word_index = tokenizer.word_index \n",
    "print('Found %s unique tokens.'%len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training data (84152, 1000)\n",
      "shape of training labels (84152, 2)\n",
      "shape of testing data (21038, 1000)\n",
      "shape of testing labels (21038, 2)\n"
     ]
    }
   ],
   "source": [
    "#preparing training data\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "X_train = pad_sequences(sequences, maxlen = MAX_SEQUENCE_LENGTH)#长度不足1000的用0填充(前端填充)\n",
    "X_test = pad_sequences(sequences_test, maxlen = MAX_SEQUENCE_LENGTH) \n",
    "\n",
    "\n",
    "y_train = to_categorical(label_train) \n",
    "y_test = to_categorical(label_test)\n",
    "\n",
    "print('shape of training data',X_train.shape)\n",
    "print('shape of training labels',y_train.shape)\n",
    "print('shape of testing data',X_test.shape)\n",
    "print('shape of testing labels',y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 16830 validation samples \n"
     ]
    }
   ],
   "source": [
    "# split the training data for fun or simply use the paremeter in fit(validation_split=0.2)\n",
    "index = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(index)\n",
    "X_train = X_train[index]\n",
    "y_train = y_train[index]\n",
    "num_validation_samples = int(0.2*X_train.shape[0])\n",
    "print('split %d validation samples '%num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of real training data (67322, 1000)\n",
      "shape of real training labels (67322, 2)\n",
      "shape of validatation training data (16830, 1000)\n",
      "shape of validatation training labels (16830, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train_split = X_train[:-num_validation_samples]\n",
    "y_train_split = y_train[:-num_validation_samples]\n",
    "X_train_val = X_train[-num_validation_samples:]\n",
    "y_train_val = y_train[-num_validation_samples:]\n",
    "\n",
    "print('shape of real training data',X_train_split.shape)\n",
    "print('shape of real training labels',y_train_split.shape)\n",
    "print('shape of validatation training data',X_train_val.shape)\n",
    "print('shape of validatation training labels',y_train_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embedding matrix: (20001, 100)\n"
     ]
    }
   ],
   "source": [
    "#embedding matrix\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "num_words = min(MAX_NUM_WORDS,len(word_index))\n",
    "embedding_matrix = np.zeros((num_words +1,EMBEDDING_DIM))\n",
    "for word,i in word_index.items():\n",
    "    if i>MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = words_index.get(word) #array\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector        \n",
    "print('shape of embedding matrix:',embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Model completed ：）\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1000, 100)         2000100   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 4         \n",
      "=================================================================\n",
      "Total params: 2,080,605\n",
      "Trainable params: 2,000,205\n",
      "Non-trainable params: 80,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model LSTM\n",
    "\n",
    "embedding_layer = Embedding(num_words + 1, \n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=MAX_SEQUENCE_LENGTH, \n",
    "              )\n",
    "print('Building model...')\n",
    "\n",
    "model = Sequential() \n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))  #100维\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(len(all_train['classes']), activation='softmax'))\n",
    "model.layers[1].trainable=False\n",
    "\n",
    "print('Model completed ：）')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n",
    "model.compile(\n",
    "            optimizer='adam',#优化器\n",
    "            loss='binary_crossentropy',#损失函数\n",
    "            metrics=['accuracy'],#指标列表\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 67322 samples, validate on 16830 samples\n",
      "Epoch 1/5\n",
      "67322/67322 [==============================] - 623s 9ms/step - loss: 0.6903 - acc: 0.5456 - val_loss: 0.6895 - val_acc: 0.5811\n",
      "Epoch 2/5\n",
      "67322/67322 [==============================] - 727s 11ms/step - loss: 0.6889 - acc: 0.5623 - val_loss: 0.6883 - val_acc: 0.5996\n",
      "Epoch 3/5\n",
      "67322/67322 [==============================] - 684s 10ms/step - loss: 0.6873 - acc: 0.5799 - val_loss: 0.6865 - val_acc: 0.6149\n",
      "Epoch 4/5\n",
      "67322/67322 [==============================] - 580s 9ms/step - loss: 0.6851 - acc: 0.5954 - val_loss: 0.6839 - val_acc: 0.6272\n",
      "Epoch 5/5\n",
      "67322/67322 [==============================] - 576s 9ms/step - loss: 0.6815 - acc: 0.6130 - val_loss: 0.6797 - val_acc: 0.6349\n",
      "21038/21038 [==============================] - 59s 3ms/step\n",
      "Loss: 0.6835604953963059\n",
      "Accuracy: 0.6002947026666257\n"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "batch_size = 1000\n",
    "model.fit(X_train_split, y_train_split, batch_size=batch_size, epochs=5, validation_data=(X_train_val,y_train_val))\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Loss:',loss) \n",
    "print('Accuracy:',acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotted!\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model.png')\n",
    "print('plotted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
